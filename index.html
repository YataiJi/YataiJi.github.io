<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">
    <title>Homepage</title>
    <meta name="author" content="Yatai Ji">
    <link rel="shortcut icon" href="img/favicon.jpg">
    <link rel="stylesheet" href="css/index.css">
    <link rel="stylesheet" href="css/all.min.css">
    <meta name="generator" content="Hexo 5.4.0">
</head>
<body>
    <header id="page_header">
        <div class="header_wrap">
            <div id="blog_name">
                <a class="blog_title" id="site-name" href="https://YataiJi.github.io"> Homepage</a>
            </div>
            <button class="menus_icon">
                <div class="navicon"></div>
            </button>
            <ul class="menus_items">
                <li class="menus_item">
                    <a class="site-page" href="https://YataiJi.github.io#Publications"> Publications</a>
                </li>
                <li class="menus_item">
                <a class="site-page" href="https://www.zhihu.com/people/yin-qian-shang-69" target="_blank" rel="noopener"> Blog</a>
                </li>
            </ul>
        </div>
    </header>
    <main id="page_main">
        <div class="side-card sticky">
            <div class="card-wrap" itemscope itemtype="http://schema.org/Person">
                <div class="author-avatar">
                    <img class="avatar-img" src="img/photo.jpg" onerror="this.onerror=null;this.src='img/photo.png'" alt="avatar">
                </div>
                <div class="author-discrip">
                    <h3>Yatai Ji</h3>
                    <p class="author-bio">PH.D. Student in MMLab, HKU</p>
                </div>
                <div class="author-links is-close">
                    <button class="btn m-social-links">Links</button>
                    
                    <ul class="social-icons">
                        <li>
                            <a class="social-icon" href="https://scholar.google.com.hk/citations?hl=zh-CN&user=O0gthJQAAAAJ" target="_blank">
                                <i class="fas fa-graduation-cap" aria-hidden="true"></i>
                            </a>
                        </li>

                        <li>
                            <a class="social-icon" href="https://github.com/jiyt17" target="_blank">
                                <i class="fab fa-github" aria-hidden="true"></i>
                            </a>
                        </li>

                        <li>
                            <a class="social-icon" href="mailto://jyt21@mails.tsinghua.edu.cn" target="_blank">
                                <i class="fas fa-envelope" aria-hidden="true"></i>
                            </a>
                        </li>

                        <li>
                            <a class="social-icon" href="https://www.zhihu.com/people/yin-qian-shang-69" target="_blank">
                                <i class="fas fa-blog" aria-hidden="true"></i>
                            </a>
                        </li>
                    </ul>


                    <ul class="social-links">
                        <li>
                            <a class="e-social-link" href="https://scholar.google.com.hk/citations?hl=zh-CN&user=O0gthJQAAAAJ" target="_blank">
                                <i class="fas fa-graduation-cap" aria-hidden="true"></i>
                                <span> Google Scholar</span>
                            </a>
                        </li>

                        <li>
                            <a class="e-social-link" href="https://github.com/jiyt17" target="_blank">
                                <i class="fab fa-github" aria-hidden="true"></i>
                                <span> Github</span>
                            </a>
                        </li>

                        <li>
                            <a class="e-social-link" href="mailto://jyt21@mails.tsinghua.edu.cn" target="_blank">
                                <i class="fas fa-envelope" aria-hidden="true"></i>
                                <span> Email</span>
                            </a>
                        </li>

                        <li>
                            <a class="e-social-link" href="https://www.zhihu.com/people/yin-qian-shang-69" target="_blank">
                                <i class="fas fa-blog" aria-hidden="true"></i>
                                <span> Zhihu</span>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="page" itemscope itemtype="http://schema.org/CreativeWork">
            <article>
                <h1 id="Biography">
                    <a href="#Biography" class="headerlink" title="Biography"></a>
                    <font color=#336699>Biography</font>
                </h1>
                <p>
                    I am now a PH.D student in <a href="https://mmlab-hku.com/", target="_blank">MMLab of HKU</a>, supervised by Prof. <a href="http://luoping.me/", target="_blank">Ping Luo</a>. 
		    During my master period, I studied in <a href="https://sites.google.com/view/iigroup-thu", target="_blank">IIG group</a> in Tsinghua University, 
				    supervised by Prof. <a href="https://scholar.google.com/citations?user=4gH3sxsAAAAJ&hl=zh-CN", target="_blank">Yujiu Yang</a>. 
                    I received my bachelor degree in Department of Automation from Tsinghua University in 2021.
                    My research interests lie in Multi-Modal Learning, including Vision Language Pre-training and Large Multimodal Model. Recently, I have some works on both large vision-language model and visual generation.
                </p>
                <br/>

                <h1 id="Recent-News">
                    <a href="#Recent-News" class="headerlink" title="Recent News"></a>
                    <font color=#336699>Recent News</font>
                </h1>
                <!--<ul>
                    <li>2022.11 - One research paper is accepted by AAAI 2023.</li>
                    <li>2022.07 - We are 3rd in ACM MM 2022 <a href="http://www.picdataset.com/challenge/task/mtvg/", target="_blank">Make-up Temporal Video Grounding (MTVG)</a>.</li>
                    <li>2022.06 - We are 1st in CVPR 2022 <a href="https://www.aicrowd.com/challenges/cvpr-2022-clear-challenge", target="_blank">Continual LEArning on Real-World Imagery (CLEAR)</a>.</li>   
                </ul>-->
                <h1 id="Publications">
                    <a href="#Publications" class="headerlink" title="Publications"></a>
                    <font color=#336699>Selected Publications</font>
                </h1>
                <ul>
		    <li>
                        <p>
                            <font color=#336699>IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model</font>
                        </p>
                        <ul>
                            <li>
                                <b>Yatai Ji</b>, Shilong Zhang, Jie Wu, Peize Sun, Weifeng Chen, Xuefeng Xiao, Sidi Yang, Yujiu Yang, Ping Luo.
                            </li>
                            <li>
                                <em>Arxiv</em>
                                [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.07577">pdf</a>] 
                                [<a target="_blank" rel="noopener" href="https://github.com/jiyt17/IDA-VLM">code</a>]
                            </li>
                        </ul>
                    </li>
		    <li>
                        <p>
                            <font color=#336699>Control-A-Video: Controllable Text-to-Video Diffusion Models with Motion Prior and Reward Feedback Learning.</font>
                        </p>
                        <ul>
                            <li>
                                Weifeng Chen<sup>*</sup>, <b>Yatai Ji</b><sup>*</sup>, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, Liang Lin.
                            </li>
                            <li>
                                <em>Arxiv</em>
                                [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.13840">pdf</a>] 
                                [<a target="_blank" rel="noopener" href="https://github.com/Weifeng-Chen/control-a-video">code</a>]
                            </li>
                        </ul>
                    </li>
                    <li>
                        <p>
                            <font color=#336699>Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning</font>
                        </p>
                        <ul>
                            <li>
                                <b>Yatai Ji</b><sup>*</sup>, 
                                Rongcheng Tu<sup>*</sup>, Jie Jiang, Weijie Kong, Chengfei Cai, Wenzhe Zhao, Hongfa Wang, Yujiu Yang, Wei Liu.
                            </li>
                            <li>
                                <em>CVPR2023</em>
                                 (CCF A, research paper)
                                [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.13437.pdf">pdf</a>] 
                                [<a target="_blank" rel="noopener" href="https://github.com/IIGROUP/SCL">code</a>]
                            </li>
                        </ul>
                    </li>
                    <li>
			<p>
                            <font color=#336699>MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model</font>
                        </p>
                        <ul>
                            <li>
                                <b>Yatai Ji</b><sup>*</sup>, 
                                Junjie Wang<sup>*</sup>, Yuan Gong, Lin Zhang, Yanru Zhu, Hongfa Wang, Jiaxing Zhang, Tetsuya Sakai, Yujiu Yang.
                            </li>
                            <li>
                                <em>CVPR2023</em>
                                 (CCF A, research paper)
                                [<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_MAP_Multimodal_Uncertainty-Aware_Vision-Language_Pre-Training_Model_CVPR_2023_paper.pdf">pdf</a>] 
                                [<a target="_blank" rel="noopener" href="https://github.com/IIGROUP/MAP">code</a>]
                            </li>
                        </ul>
                    </li>
                    <li>
			<p>
                            <font color=#336699>MIRTT: Learning Multimodal Interaction Representations from Trilinear Transformers for Visual Question Answering</font>
                        </p>
                        <ul>
                            <li>
                                Junjie Wang <sup>*</sup>, 
                                <b>Yatai Ji</b><sup>*</sup>, 
                                Jiaqi Sun, Yujiu Yang, Tetsuya Sakai.
                            </li>
                            <li>
                                <em>EMNLP2021 findings</em>
                                 (CCF B, research paper)
                                [<a target="_blank" rel="noopener" href="https://aclanthology.org/2021.findings-emnlp.196/?ref=https://githubhelp.com">pdf</a>] 
                                [<a target="_blank" rel="noopener" href="https://github.com/IIGROUP/MIRTT">code</a>]
                            </li>
                        </ul>
                    </li>


                </ul>

                <h1 id="Awards">
                    <a href="#Awards" class="headerlink" title="Awards"></a>
                    <font color=#336699>Awards</font>
                </h1>
                <ul>
                    <li>2023.7, Tencent Rhino-Bird Research Scholarship</li>
	 	    <li>2024.6, Outstanding Masterâ€™s Thesis Award of Tsinghua University</li>
		    <li>2024.9, Shenzhen Universiade International Scholarship</li>
                </ul>

                <h1 id="Internship">
                    <a href="#Interns" class="headerlink" title="Internship"></a>
                    <font color=#336699>Internship</font>
                </h1>
                <ul>
                    <li>2022.6~2023.7, AMAI, Department of Data Platform, Tencent</li>
		    <li>2023.9~2024.8, AI Platform, Intelligence Creation Department, ByteDance</li>
                </ul>

<!--                 <h1 id="Professional Activities">
                    <a href="#Activities" class="headerlink" title="Professional Activities"></a>
                    <font color=#336699>Professional Activities</font>
                </h1>
                <ul>
                    <li>Conference Reviewer: ACM MM.</li>
		    <li>Journal Reviewer: Pattern Recognition (PR).</li>
                </ul> -->

                <p>(Last updated on September, 2024)</p>
            </article>
        </div>
    </main>
    <div class="nav-wrap">
        <div class="nav">
            <button class="site-nav">
                <div class="navicon"></div>
            </button>
            <ul class="nav_items">
                <li class="nav_item">
                    <a class="nav-page" href="/#Publications"> Publications</a>
                </li>
            </ul>
        </div>
        <div class="cd-top">
            <i class="fa fa-arrow-up" aria-hidden="true"></i>
        </div>
    </div>
    <footer id="page_footer">
        <div class="footer_wrap">
            <div class="copyright">&copy;2020 - 2022 by Yatai Ji. </div>
            <div class="theme-info">
                Powered by 
                <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a>
                 & 
                <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a>
            </div>
        </div>
    </footer>
    <script src="js/jquery.min.js"></script>
    <script src="js/jquery.pjax.min.js"></script>
    <script src="js/main.js"></script>
</body>
</html>
